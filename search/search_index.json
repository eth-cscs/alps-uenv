{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"CSCS uenv","text":"<p>This site provides the following documentation for users of uenv on Alps:</p> <ul> <li>uenv provided on Alps.</li> <li>tutorials on using uenv.</li> </ul>"},{"location":"#uenv-developers","title":"uenv developers","text":"<p>CSCS staff developing uenv can find the following best practices and pracitical guides for developing uenv.</p>"},{"location":"developers-overview/","title":"Documentation for uenv developers","text":"<p>The documentation here is a collection of best practices and practical how-to for folk writing uenv recipes, specifically for the Alps targets at CSCS.</p> <ul> <li>A practical recipe writing guide;</li> <li>A step-by-step guide for supporting ReFrame tests in the CSCS regression testing framework;</li> <li>A guide on how to write uenv documentation for this site.</li> </ul>"},{"location":"pkg-application-tutorial/","title":"Application Packaging Tutorial","text":"<p>Target audience</p> <p>The target audience for this tutorial is CSCS staff who want to provide an application or programming environment uenv for users on Alps. We assume that you are familiar with Spack and how to build uenv using Stackinator - the focus here will be on best practices.</p> <p>This tutorial walks through configuring and maintaining a uenv recipe and deployment a representative HPC application. The uenv has to support some common use cases, and should cover most of the aspects of deploying your own uenv.</p> <p>Arbor is a scientific software for neuroscience simulation, with features including:</p> <ul> <li>A C++ library with a Python interface</li> <li>Distributed execution through MPI</li> <li>Multicore simulation</li> <li>Support for both NVIDIA and AMD GPUs</li> </ul>"},{"location":"pkg-application-tutorial/#requirements","title":"Requirements","text":"<p>Before starting, we gather requirements for the use cases of the uenv on the system, in order to understand:</p> <ul> <li>which packages the uenv will provide;</li> <li>which interfaces the uenv will provide to those packages.</li> </ul>"},{"location":"pkg-application-tutorial/#supported-workflows","title":"Supported workflows","text":"<p>For Arbor we wish to support two workflows:</p> <ul> <li>Application: provide optimised builds of Arbor for users to use directly</li> <li>BYO/Developer: Arbor is under active development, and some users require the ability to build the latest bleeding edge version, or build with a customised configuration. This workflow also supports Arbor developers, who need to implement new features, fix bugs and test on Alps.</li> </ul>"},{"location":"pkg-application-tutorial/#supported-systems","title":"Supported systems","text":"<p>Arbor is well-optimised for both CPU and GPU execution and users of systems with and without accelerators, so we will provide it for the following platforms:</p> <ul> <li>multicore: <code>zen2</code>/<code>zen3</code></li> <li><code>gh200</code></li> </ul> <p>supported platforms</p> <p>Supported targets on Alps are currently <code>zen2</code>, <code>zen3</code>, <code>a100</code>, <code>mi200</code>, and <code>gh200</code>.</p> <p>For more information, see the internal CSCS confluence. Also, for information about which targets are available on which vClusters, see the <code>config.yaml</code>.</p>"},{"location":"pkg-application-tutorial/#compilers","title":"Compilers","text":"<p>Arbor is a C++17 library that officially supports GCC and Clang, with a Python front end.</p> <p>For this we choose the following compiler versions:</p> target compiler cuda python <code>zen2</code>/<code>zen3</code> <code>gcc@13.2</code> - <code>python@3.11</code> <code>gh200</code> <code>gcc@13.2</code> <code>cuda@12.4</code> <code>python@3.11</code>"},{"location":"pkg-application-tutorial/#packages","title":"Packages","text":"<p>The first step when building an application, use-case or workflow uenv is to determine which specs to add to the list.</p> <p>If the aim was to provide arbor with cuda and Python support enabled, an <code>environments.yaml</code> file that provides a single spec <code>arbor@0.9 +python</code> could be sufficient, e.g.:</p> simple environments.yaml<pre><code>arbor:\n  compiler:\n      - toolchain: gcc\n        spec: gcc\n  mpi:\n      spec: cray-mpich\n  unify: true\n  specs:\n  - arbor@0.9 +python\n  variants:\n  - +mpi\n  - +cuda\n  - cuda_arch=90\n  views:\n    arbor:\n      links: root\n</code></pre> <p>This environment definition will build arbor, with all of its dependencies implicitly concretised by Spack. Such a simple recipe is sometimes sufficient, however one will often need to provide a more detailed set of specs. Reasons for more detailed specs include:</p> <ul> <li>to pin the version of a specific dependency, e.g.:<ul> <li>to ensure that the version of a package is not dependent on which version of Spack is used;</li> <li>to a version that is well supported and tested on the target system;</li> <li>to a version that patches a bug on the target system.</li> </ul> </li> <li>to specialise the spec of a specific dependency, e.g.:<ul> <li>with non-default variants that support all features on the target system;</li> <li>with non-default variants that give the best performance on the target system;</li> <li>to use a specific compiler when more than one compiler toolchain is used to build packages in an environment.</li> </ul> </li> <li>to explicitly list all of the dependencies to provide to users in an environment view</li> </ul> <p>The objective for the Arbor uenv is to provide both Arbor and all of the tools and libraries to \"build your own\" Arbor. This requires providing all of the libraries and tools required to download the Arbor source code, run CMake, and build in a file system view.</p> <p>As a starting point, we use the spack package for Arbor. From this we derive a list of dependencies:</p> <ul> <li>direct dependencies like <code>pugixml</code>, <code>fmt</code> and <code>pybind11</code> needed to build Arbor.</li> <li>compiler and languages like <code>python</code> and <code>cuda</code>.</li> </ul>"},{"location":"pkg-application-tutorial/#the-recipe","title":"The Recipe","text":"<p>With requirements in hand, it is now time to write the recipe.</p>"},{"location":"pkg-application-tutorial/#config","title":"Config","text":"<p>There are a few simple choices to make when writing the <code>config.yaml</code> file:</p> <code>name</code> <p>Keep it simple, we choose <code>arbor</code>.</p> <p>Tip</p> <p>Use the same name on all hardware targets, i.e. use a name like <code>arbor</code> or <code>gromacs</code> instead of <code>arbor-gpu</code> or <code>gromacs-x86</code>. By doing this users can more easily find your uenv on all vClusters - if they are on a system with an x86 CPU, they can assume that the <code>arbor</code> uenv has been built appropriately.</p> <p>The uenv CLI tool also allows users to disambiguate which micro-architecture they require, if on a system that provides versions of a uenv built for multiple uarch:</p> <pre><code>uenv image ls --uarch=gh200 arbor\nuenv image ls --uarch=zen2 arbor\n</code></pre> <code>spack</code> <p>By default use the most recent version of Spack supported by Stackinator. At the time of writing, the most recent version of Spack is <code>v0.21</code>, for which it is recommended to use the <code>releases/v0.21</code> branch, which receives backports of bug fixes while not changing the API or recipe definitions.</p> <p>Warning</p> <p>The <code>develop</code> branch should be avoided for uenv deployed on CSCS clusters unless it is absolutely necessary.</p> <code>mount</code> <p>Normally application and development uenvs go in <code>/user-environment</code> and tools that you might want to use alongside a development or application uenv go in <code>/user-tools</code> (e.g. a debugger). For Arbor, we choose the default <code>/user-environment</code> path.</p> <code>description</code> <p>Keep it simple, fit it on one line.</p> <code>mc</code><code>gh200</code> config.yaml<pre><code>\n</code></pre> config.yaml<pre><code>\n</code></pre>"},{"location":"pkg-application-tutorial/#compilers_1","title":"Compilers","text":"<p>Based on our requirements above, defining compilers is straightforward.</p> <code>mc</code><code>gh200</code> compilers.yaml<pre><code>\n</code></pre> compilers.yaml<pre><code>\n</code></pre>"},{"location":"pkg-application-tutorial/#environments","title":"Environments","text":"<p>The environment definitions include the specs that we want to provide to end users, and the relevant <code>cuda</code> and <code>python</code> versions depending on each application.</p> <code>mc</code><code>gh200</code> environments.yaml<pre><code>\n</code></pre> environments.yaml<pre><code>\n</code></pre> <p>variants</p> <p>Environments on GH200 will typically have the following variants set:</p> <ul> <li><code>+cuda</code> sets that variant for all that support it, required for NVIDIA GPU builds.</li> <li><code>cuda_arch=90</code> is required for <code>gh200</code> (use <code>cuda_arch=80</code> for the <code>a100</code> nodes)</li> </ul> <p>views and roots</p> <p>Always use <code>view:link:roots</code> if possible to filter which packages are added to views. The default <code>all</code> setting and also the <code>run</code> setting can add a lot of packages that were not explicitly in the list of your uenv's specs.</p> <p>Packages in the view can lead to conflicts, which can be avoided by only including packages that are strictly required. For example, if a view has a common dependency like <code>libssl</code> in its <code>/lib</code> path, and <code>LD_LIBRARY_PATH</code> is set, system CLI tools like <code>git</code> can crash because the link against the <code>libssl</code> in the uenv at runtime.</p>"},{"location":"pkg-application-tutorial/#modules","title":"Modules","text":"<p>We add a module file, which controls which modules are provided by the uenv. This is because some users might want modules, and it doesn't hurt to provide them (this is a weak reason, and we accept that we will be on the hook for supporting them for users who incorporate them into their workflows).</p> <p>Info</p> <p>If you don't need to provide modules, set <code>modules: False</code> in <code>config.yaml</code>.</p> mcgh200 modules.yaml<pre><code>\n</code></pre> modules.yaml<pre><code>\n</code></pre>"},{"location":"pkg-application-tutorial/#testing","title":"Testing","text":"<p>Failure</p> <p>write reframe tests.</p>"},{"location":"pkg-application-tutorial/#deployment","title":"Deployment","text":""},{"location":"pkg-application-tutorial/#configuring-the-pipeline","title":"Configuring the pipeline","text":"<p>The target systems for deploying the Arbor uenv to users are Eiger (<code>zen2</code>) and Santis (<code>gh200</code>).</p> <p>To enable the CI/CD pipeline to build and deploy the uenv on these systems, update the <code>config.yaml</code> file in the alps-uenv repository:</p> <pre><code>uenvs:\n  arbor:\n    v0.9:\n      recipes:\n        zen2: v0.9/mc\n        gh200: v0.9/gh200\n      deploy:\n        eiger: [zen2]\n        santis: [gh200]\n</code></pre> <p>Tip</p> <p>To test that the pipeline yaml is correctly configured before pushing the changes and making a PR, you can run a basic test with the new uenv:</p> <pre><code>system=santis uarch=gh200 uenv=arbor:v0.9 ./ci/configure-pipeline\nsystem=eiger uarch=zen2 uenv=arbor:v0.9 ./ci/configure-pipeline\n</code></pre> <p>If there are no obvious error messages, you are good to go!</p>"},{"location":"pkg-application-tutorial/#running-the-pipeline","title":"Running the pipeline","text":"<p>To run the pipeline that will automatically build and test your uenv, first create a PR:</p> <ol> <li>Push your changes to a branch (preferably in a fork of the main alps-uenv repository).</li> <li>Open a PR with your changes.</li> </ol> <p>Once the PR is created, the pipeline has to be triggered for each individual combination of uenv/version/uarch/vCluster by using a specially formatted</p> <pre><code>cscs-ci run alps;system=eiger;uarch=zen2;uenv=arbor:v0.9\ncscs-ci run alps;system=santis;uarch=gh200;uenv=arbor:v0.9\n</code></pre>"},{"location":"pkg-application-tutorial/#checking-the-build","title":"Checking the build","text":"<p>Log onto the target system, e.g. <code>santis</code>, and use the <code>uenv image find --build</code> command to search for the build.</p> <pre><code>&gt; uenv image find --build arbor\nuenv/version:tag                        uarch date       id               size\narbor/v0.9:1250847801                   gh200 2024-04-12 89c9a36f21b496a2 3.6GB\narbor/v0.9:1249535229                   gh200 2024-04-11 0a2d82448ecaafd7 3.6GB\n</code></pre> <p>Info</p> <p>The <code>--build</code> flag is required with the <code>find</code> and <code>pull</code> commands to interact with images that have been built by the pipeline, but not yet deployed.</p> <p>Pick the version to pull (if it isn't clear which version to pull, inspect the logs of the CI/CD job that built the image).</p> <pre><code># pull the image using its id\nuenv image pull --build 89c9a36f21b496a2\n\n# then start the image to test it\nuenv image start 89c9a36f21b496a2\n</code></pre>"},{"location":"pkg-application-tutorial/#docs","title":"Docs","text":"<p>Failure</p> <p>Write about how to document.</p>"},{"location":"pkg-reframe/","title":"ReFrame Testing Tutorial","text":"<p>When ReFrame tests are enabled for a uenv, they are automatically run:</p> <ul> <li>in the CI/CD pipeline after the image has been built;</li> <li>in daily/weekly testing of individual vClusters;</li> <li>and when upgrading and updating vClusters.</li> </ul> <p>This page is a tutorial that will guide you through the process of enabling testing for your uenv, and on writing portable tests that will run on any uenv-enabled system on Alps.</p> <p>Info</p> <p>Currently this tutorial shows how to create and run tests for a uenv image that has already been created.</p> <p>TODO:</p> <ul> <li>Describe how to add the test to the recipe and view the test results in the CI/CD pipeline</li> </ul>"},{"location":"pkg-reframe/#how-uenv-reframe-testing-works","title":"How uenv ReFrame testing works","text":"<p>CSCS maintains a set of ReFrame tests in the CSCS ReFrame tests repository eth-cscs/cscs-reframe-tests. These tests cover a very wide range of features, including application tests, login node health and Slurm, and can be run on any vCluster on Alps.</p> <p>Info</p> <p>When running this test suite with a uenv, only the subset of the test suite that is relevant for the uenv will be run.</p> <p>Setting up tests for a uenv requires making changes to two repositories:</p> <ul> <li>eth-cscs/alps-uenv adding metadata to the uenv to be used by ReFrame to:<ul> <li>load the uenv and configure the environment so that it is ready to run tests;</li> <li>and, choose which tests from the test suite are used to test the uenv.</li> </ul> </li> <li>eth-cscs/cscs-reframe-tests updating and adding tests in the that are relevant to the uenv.<ul> <li>might not be necessary if the tests already exist.</li> </ul> </li> </ul>"},{"location":"pkg-reframe/#uenv-recipe-meta-data","title":"uenv recipe meta data","text":"<p>To enable ReFrame tests in your uenv, a yaml file <code>extra/reframe.yaml</code> should be added to the recipe.</p> <p>Below is an example <code>reframe.yaml</code> file:</p> extra/reframe.yaml for testing a single environment provided by a uenv<pre><code>develop:\n  features:\n    - cuda\n    - mpi\n    - arbor-dev\n  cc: mpicc\n  cxx: mpic++\n  ftn: mpifort\n  views:\n    - develop\n</code></pre> <p>This configuration defines a single environment named <code>develop</code>, which corresponds a ReFrame environment, with some additions.</p> <ul> <li><code>features</code>: a list of ReFrame features that are provided by the environment.<ul> <li>used to decide which tests will be run against the uenv.</li> <li>in this case the uenv provides:<ul> <li><code>cuda</code>: expect tests that compile and test NVIDIA GPU aware problems.</li> <li><code>mpi</code>: expect basic MPI tests that compile and validate MPI to be run. When combined with <code>cuda</code> above, tests for GPU-aware MPI will be run.</li> <li><code>arbor-dev</code>: a specific feature that specifies that the environment provides everything required to build and run arbor.</li> </ul> </li> </ul> </li> <li><code>cc</code>, <code>cxx</code>, <code>ftn</code>: define the compiler aliases<ul> <li>see the ReFrame environments documentation.</li> </ul> </li> <li><code>views</code>: (optional) a list of views to load.<ul> <li>in this case <code>develop</code> view is to be loaded.</li> </ul> </li> </ul> <p>A uenv can provide multiple views, for different use cases. The most common example is a uenv that provides two views: one that provides an application, and another that provides the tools used to build the application. Another example is the <code>modules</code> and <code>spack</code> views, that expose a module interface or useful configuration for using Spack with the uenv.</p> <p>Similarly, it is possible to create multiple environments to test. The example below defines two environments that provide the same features, i.e. the same tests will be run on each. The first example is the one above, and the second sets up an equivalent environment using modules. This would be useful for a uenv that has some users who insist on using modules to set up their build environment.</p> extra/reframe.yaml for multiple environments to test<pre><code>develop:\n  features:\n    - cuda\n    - mpi\n    - arbor-dev\n  cc: mpicc\n  cxx: mpic++\n  ftn: mpifort\n  views:\n    - develop\nmodules:\n  features:\n    - cuda\n    - mpi\n    - arbor-dev\n  cc: mpicc\n  cxx: mpic++\n  ftn: mpifort\n  views:\n    - modules\n  activation:\n    - module load cuda cray-mpich cmake ninja fmt\n</code></pre> <p>Info</p> <p>The <code>modules</code> environment uses an additional <code>activation</code> field, which is a series of commands to run in the shell before running the tests. In this case, modules provided by the uenv are loaded.</p> <p>What is an environment?</p> <p>What is the difference between using <code>module load</code>, activating a python venv, loading a Spack environment, or a uenv view?</p> <p>Nothing!</p> <p>They all do the same thing - set environment variables.</p> <p>The main variables that change the behavior of the system are <code>PATH</code> and <code>LD_LIBRARY_PATH</code>, though there are many others like <code>PKG_CONFIG_PATH</code>, <code>CUDA_HOME</code>, <code>MODULEPATH</code> etc that will have more subtle effects on configuring and building software.</p> <p>Configuring an environment for running tests requires specifying the commands that will modify and set environment variables such that the tests can run. For example, a view or module might be loaded to make the executable of a scientific code be in <code>PATH</code>, or to add tools like <code>cmake</code>, <code>nvcc</code> and <code>gcc</code> to <code>PATH</code> so that we can run a test that builds an application.</p>"},{"location":"pkg-reframe/#creating-uenv-tests","title":"Creating uenv tests","text":"<p>The final objective for adding tests to a uenv is to have:</p> <ol> <li>a uenv deployed with an <code>extra/reframe.yaml</code> file;</li> <li>and tests in the eth-cscs/cscs-reframe-tests repository</li> </ol> <p>In this second half of the tutorial, a workflow for doing this that minimises the amount of time spent waiting in job and CI/CD queues is provided. Before starting, you will need the following:</p> <ul> <li>a working uenv squashfs image with corresponding meta data path;</li> <li>and a list of which tests you want to run against the uenv (some of which you have yet to write).</li> </ul>"},{"location":"pkg-reframe/#step-1-create-a-reframeyaml-file","title":"Step 1: create a reframe.yaml file","text":"<pre><code># pull the image that you want to start developing tests for, e.g.:\n$ uenv image pull cp2k/2024.2:v1\n\n# get the meta data path\n$ meta=$(uenv image inspect cp2k/2024.2:v1 --format={meta})\n\n# check the path - your location will be different\n$ echo ${meta}\n\n# create the reframe meta data file\n$ mkdir -p ${meta}/extra\n$ vim ${meta}/extra/reframe.yaml\n</code></pre> <p>The <code>meta</code> path is the metadata for the uenv for the image.</p> why create reframe.yaml in this location? <p>We inject the <code>reframe.yaml</code> file into the meta data in the uenv repo to create a \"development environment\", where it can be modified while developing the tests in an interactive shell.</p> <p>The <code>reframe.yaml</code> file will be added to the recipe later, once it is time to start testing in a CI/CD pipeline.</p>"},{"location":"pkg-reframe/#step-2-set-up-the-cscs-reframe-tests","title":"Step 2: set up the CSCS reframe tests","text":"<p>The next step is to check out and setup ReFrame and the CSCS ReFrame test suite.</p> <p>It might be a good idea to create a path for this work, and cloning the ReFrame and CSCS test suite repos as sub-directories.</p>"},{"location":"pkg-reframe/#set-up-reframe","title":"Set up ReFrame","text":"<p>The first step is to download and set up ReFrame:</p> <ul> <li>clone from ReFrame GitHub repository</li> <li>run the bootstrap process that installs ReFrame's dependencies;</li> <li>then add reframe to <code>PATH</code>.</li> </ul> <pre><code># clone from repo\n$ git clone git@github.com:reframe-hpc/reframe.git\n\n# run bootstrap process (only needs to be done once)\n$ cd reframe\n$ ./bootstrap.sh\n\n# add to PATH and verify that everything works\n$ export PATH=$PWD/bin:$PATH\n$ reframe --version\n</code></pre>"},{"location":"pkg-reframe/#set-up-the-reframe-tests","title":"Set up the ReFrame tests","text":"<p>The next step is to clone the CSCS test suite, and create a new branch where we will make any changes required to test our uenv.</p> <pre><code># download the reframe tests\n$ git clone -b alps git@github.com:eth-cscs/cscs-reframe-tests.git\n$ cd cscs-reframe-tests\n\n# start a branch for developing the tests (choose your own appropriate name)\ngit switch -c uenv-arbor\n</code></pre> <p>Tip</p> <p>Always create your working branch off of the <code>alps</code> branch. The <code>alps</code> branch is used for tests run on Alps vClusters. It will become the main branch, once Piz Daint is decommisioned.</p>"},{"location":"pkg-reframe/#addingupdating-tests-to-a-uenv","title":"Adding/updating tests to a uenv","text":"<p>Now everything is in place to implement the tests for your uenv, which will involve one or two of the following:</p> <ol> <li>add some new tests;</li> <li>or, updating / porting existing tests that were written for the CPE+EasyBuild builds on Daint XC or Eiger.</li> </ol> <p>There are no existing tests for Arbor, so let's start by creating a path for the tests we will write:</p> <pre><code># create a path for the checks\nmkdir checks/apps/arbor\ncd checks/apps/arbor\n</code></pre> <p>Note</p> <p>If there are existing tests, start editing the existing tests directly, or create a new test in that path.</p> <p>In this tutorial we will write tests that:</p> <ol> <li>build Arbor, including unit tests and example miniapps</li> <li>run Arbor's unit tests</li> <li>run a benchmark with a single MPI rank on one GH200 GPU</li> <li>run a benchmark with 4 ranks and 4 GPUs on a GH200 node.</li> </ol> <p>Link to the tests.</p> <p>Note</p> <p>These tests use the packages and tools in the uenv (CMake, cray-mpich, Python, cuda, etc) to build Arbor, then as runtime dependencies for running the Arbor tests.</p> <p>If your uenv provides a pre-built application, you can skip building and use the executable directly to run validation and benchmark tests.</p> <p>Here is a link to the tests for Arbor: <code>checks/apps/arbor/arbor-dev.py</code>.</p> <p>The modules <code>import</code>ed at the top of the file will depend on the ReFrame features used for the tests. The <code>uenv</code> module is implemented in <code>config/utilities/uenv.py</code>:</p> <pre><code>import uenv\n</code></pre> <p>It provides the <code>uenv.uarch()</code> function, that will be used to determine the uenv uarch (<code>gh200</code>, <code>a100</code>, <code>zen2</code>, etc.) of the system where tests are running. We will see it in action below.</p>"},{"location":"pkg-reframe/#test-building-the-software","title":"Test: building the software","text":"<p>Building is handled by a test, in this case called <code>arbor_build</code>, that derives from <code>rfm.CompileOnlyRegressionTest</code>.</p> <p>Info</p> <p>Points of interest are annotated in the code below with  symbols, click on them to expand.</p> <pre><code>class arbor_build(rfm.CompileOnlyRegressionTest):\n    descr = 'Build Arbor'\n    valid_systems = ['*']\n    valid_prog_environs = ['+arbor-dev'] #(1)\n    build_system = 'CMake'\n    sourcesdir = None\n    maintainers = ['bcumming']\n    arbor_sources = fixture(arbor_download, scope='session') #(2)\n    build_locally = False #(3)\n\n    @run_before('compile')\n    def prepare_build(self):\n        self.uarch = uenv.uarch(self.current_partition) #(4)\n        self.build_system.builddir = os.path.join(self.stagedir, 'build')\n        tarball = f'v{self.arbor_sources.version}.tar.gz'\n\n        tarsource = os.path.join(self.arbor_sources.stagedir, tarball)\n        self.prebuild_cmds = [\n            f'tar --strip-components=1 -xzf {tarsource} -C {self.stagedir}'\n        ]\n\n        self.build_system.config_opts = [\n            '-DARB_WITH_MPI=on',\n            '-DARB_WITH_PYTHON=on',\n        ]\n        # set architecture-specific flags\n        if self.uarch == 'gh200': #(5)\n            self.build_system.config_opts += [\n                '-DCMAKE_CUDA_ARCHITECTURES=90',\n                '-DARB_GPU=cuda',\n            ]\n        elif self.uarch == 'a100':\n            self.build_system.config_opts += [\n                '-DCMAKE_CUDA_ARCHITECTURES=80',\n                '-DARB_GPU=cuda',\n                '-DARB_VECTORIZE=on'\n            ]\n        elif self.uarch == 'zen2':\n            self.build_system.config_opts += ['-DARB_VECTORIZE=on']\n\n        self.build_system.max_concurrency = 64\n\n        self.build_system.make_opts = ['pyarb', 'examples', 'unit']\n</code></pre> <ol> <li>Restrict this test to only run in environments that provide the <code>arbor-dev</code> feature.     This can be a list of environments, e.g. <code>['+arbor-dev+cuda', '+python']</code> would run the test in environments that provide both <code>arbor-dev</code> and <code>cuda</code>, or environments that provide <code>python</code>.</li> <li><code>arbor_download</code> is a ReFrame fixture, that handles downloading the source for Arbor.</li> <li>The build stage is performed on a compute node using a sbatch job.     Required so that the environment is configured properly, by adding the     correct flags to the script:     <pre><code>#SBATCH --uenv=...\n#SBATCH --view=...\n</code></pre></li> <li>Determine the uarch of the nodes in the current partition, which will be a string like <code>\"gh200\"</code>, <code>\"a100\"</code>, \"<code>zen2</code>\", etc.</li> <li>Use the uarch to set node-specific flags.</li> </ol> <p>Note</p> <p>At no point do we mention specific cluster names or partitions. This is different from how tests were written for Daint and Eiger in the past, where the test would include explicit mentions of systems:</p> <pre><code>if self.current_system.name in ['daint', 'dom']:\n  self.num_tasks = 576\n  self.num_tasks_per_node = 36\n</code></pre> <p>The new approach of parameterising over uarch means that the test can be configured for any vCluster with gh200 nodes.</p>"},{"location":"pkg-reframe/#test-run-the-unit-tests","title":"Test: run the unit tests","text":"<p>The C++ Arbor library provides GoogleTest unit tests that are bundled in a single executable <code>unit</code>. The tests are not MPI enabled, and take less than 30 seconds to run 1000 individual tests. As such, they are a good quick test!</p> <pre><code>@rfm.simple_test #(1)\nclass arbor_unit(rfm.RunOnlyRegressionTest):\n    descr = 'Run the arbor unit tests'\n    valid_systems = ['*']\n    valid_prog_environs = ['+arbor-dev']\n    time_limit = '5m' #(2)\n    maintainers = ['bcumming']\n    arbor_build = fixture(arbor_build, scope='environment') #(3)\n\n    @run_before('run')\n    def prepare_run(self):\n        self.executable = os.path.join(self.arbor_build.stagedir,\n                                       'build', 'bin', 'unit')\n        self.executable_opts = []\n\n    @sanity_function\n    def validate_test(self):\n        return sn.assert_found(r'PASSED', self.stdout) #(4)\n</code></pre> <ol> <li>This is the first time that we have added an annotation to a test.     This is a \"leaf\" in our set of test dependencies, run after the download     and build stages that are its dependencies have run.</li> <li>The unit tests run quickly - so set a short time limit for higher priority queuing</li> <li>The <code>arbor_build</code> stage has to be run before this test, to build the unit tests.</li> <li>Just check that the tests passed - performance checks are implemented elsewhere</li> </ol>"},{"location":"pkg-reframe/#test-single-gpu-benchmark","title":"Test: single GPU benchmark","text":"<p>Use the <code>miniring</code> benchmark provided by Arbor to check both correctness and performance.</p> <p>The tests will run the <code>busyring</code> test in two configurations on a single GPU: <code>small</code> and <code>medium</code> in this test. A separate test (see later) will also run the <code>medium</code> module on 4 GPUs.</p> <p>To test for performance, target performance values need to be provided. We create reference target values, in a dictionary outside the test class, for the following reasons:</p> <ul> <li>To ensure that the tests can be run across any <code>cluster:partition</code> with supported uarch.</li> <li>Splitting the reference data out in this way makes it simpler to add feaures like reading performance targets from file, and adding new targets for a new uarch without touching the test itself.</li> </ul> <pre><code>arbor_references = {\n    'gh200': { #(1)\n        'serial': {\n            'small': {\n                'time_run':  (9.25, -0.1, 0.1, 's'), # (2)\n            },\n            'medium': {\n                'time_run':  (35.0, -0.05, 0.05, 's'),\n            }\n        },\n        'distributed': {\n            'medium': {\n                'time_run':  (9.2, -0.05, 0.05, 's'),\n            }\n        },\n    }\n}\n</code></pre> <ol> <li>Currently, we only have Arbor performance targets for <code>gh200</code>, fields for <code>zen2</code> would be added for Eiger testing.</li> <li>These are labelled reference targets. A link will be added when it is found in ReFrame's \"documentation\".</li> </ol> <p>The test itself:</p> <pre><code>@rfm.simple_test\nclass arbor_busyring(rfm.RunOnlyRegressionTest):\n    \"\"\"\n    run the arbor busyring example in small and medium model configurations\n    \"\"\"\n    descr = 'arbor busyring model'\n    valid_systems = ['*']\n    valid_prog_environs = ['+arbor-dev'] #(1)\n    maintainers = ['bcumming']\n    model_size = parameter(['small', 'medium']) #(2)\n\n    arbor_build = fixture(arbor_build, scope='environment') #(3)\n\n    @run_before('run')\n    def prepare_run(self):\n        self.executable = os.path.join(self.arbor_build.stagedir,\n                                       'build', 'bin', 'busyring')\n        self.executable_opts = [f'busyring-input-{self.model_size}.json']\n\n        self.uarch = uenv.uarch(self.current_partition) #(4)\n        if (self.uarch is not None) and (self.uarch in arbor_references):\n            self.reference = {\n                self.current_partition.fullname:\n                    arbor_references[self.uarch]['serial'][self.model_size]\n            }\n\n    @sanity_function\n    def validate_test(self):\n        # if the meters are printed, the simulation ran to completion\n        return sn.assert_found(r'meter-total', self.stdout)\n\n    @performance_function('s')\n    def time_run(self):\n        return sn.extractsingle(r'model-run\\s+(\\S+)', self.stdout, 1, float)\n</code></pre> <ol> <li>Only run in environments that provide <code>arbor-dev</code></li> <li>The model is parameterised on model size. In practice this means that ReFrame will run    the test twice, once for each parameter.    The value of the parameter can be accessed as <code>self.model_size</code>, see the <code>prepare_run</code> method below.</li> <li>Requires the build stage.</li> <li> <p>Instead of explicitly listing performance targets for all possible    <code>system:partition</code> combinations, set the reference targets to those for the uarch of the current partition.    In other words - the performance target is set dynamically based on the architecture of the node,    instead of being hard coded using if-else statements in the test itself.</p> <ul> <li><code>self.uarch</code> is one of the alps arch: <code>\"gh200\"</code>, <code>\"zen2\"</code>, <code>\"a100\"</code>, ... etc., or <code>None</code></li> <li><code>self.current_partition.fullname</code> is the <code>vcluster:partition</code> string, for example <code>\"daint:normal\"</code> or <code>\"todi:debug\"</code>.</li> </ul> </li> </ol> <p>Note</p> <p>The test will still run, and the <code>time_run</code> results will still be reported, when <code>arbor_references</code> does not provide values for the current uarch. However, in such cases, no comparison is made and the test will pass.</p>"},{"location":"pkg-reframe/#test-mpi-tests","title":"Test: MPI tests","text":"<pre><code>slurm_config = { #(1)\n    'gh200': {\"ranks\": 4, \"cores\": 64, \"gpu\": True},\n    'zen2':  {\"ranks\": 2, \"cores\": 64, \"gpu\": False},\n}\n\n@rfm.simple_test\nclass arbor_busyring_mpi(arbor_busyring): #(2)\n    \"\"\"\n    adapt the busyring test to check paralle MPI execution\n    \"\"\"\n\n    descr = 'arbor busyring model MPI on a single node'\n    model_size = parameter(['medium']) # (3)\n\n    @run_before('run')\n    def prepare_run(self):\n        self.uarch = uenv.uarch(self.current_partition)\n        self.executable = os.path.join(self.arbor_build.stagedir,\n                                       'build', 'bin', 'busyring')\n        self.executable_opts = [f'busyring-input-{self.model_size}.json']\n\n        self.num_tasks = slurm_config[self.uarch][\"ranks\"] # (4)\n        self.num_cpus_per_task = slurm_config[self.uarch][\"cores\"]\n        if slurm_config[self.uarch][\"gpu\"]:\n            self.job.options = ['--gpus-per-task=1']\n\n        if (self.uarch is not None) and (self.uarch in arbor_references): \n            self.reference = { # (5)\n                self.current_partition.fullname:\n                    arbor_references[self.uarch]['distributed'][self.model_size]\n            }\n</code></pre> <ol> <li>Store the slurm configuration outside the test, like we did for the performance targets</li> <li>This test shares a lot with the basic <code>arbor_busyring</code> test - so inherit.     This might be code stink in this case, but it is useful for showing how to use inheritance to refine tests.</li> <li>Reduce the model size parameter to a single option <code>medium</code> (this overwrites the inherited parameter)</li> <li>Parameterise the Slurm configuration (the <code>num_tasks</code>, <code>num_cpus_per_task</code> etc magic variables) on uarch.</li> <li>Parameterise the performance target on uarch.</li> </ol>"},{"location":"pkg-reframe/#running-tests","title":"Running tests","text":"<p>To run the tests, the first thing to do is set an environment variable that will be used by the test suite to configure the environment.</p> <pre><code>$ export UENV=arbor/v0.9:v1\n</code></pre> <p>Warning</p> <p>Currently only a uenv label can be used, i.e. the image must have already been built and pulled into a local uenv repository, that is via <code>uenv image pull</code> or <code>uenv image pull --build</code>.</p> <p>We will update this feature to allow you to provide the path of the squashfs image. It will be a hard requirement that the meta data path will be in the same path as the <code>store.squashfs</code> file.</p> <p>Warning</p> <p>If the <code>UENV</code> variable is not set proprely, the <code>reframe.yaml</code> file can't be found and you will see an error: <pre><code>ERROR: failed to load configuration: problem loading the metadata from 'extra/reframe.yaml' \n</code></pre></p> <p>To run the tests use the following commands:</p> <pre><code># run the tests\n$ reframe -C cscs-reframe-tests/config/cscs.py \\\n  -c cscs-reframe-tests/checks/apps/arbor/ --keep-stage-files \\\n  -r\n\n# perform a dry run of tests\n$ reframe -C cscs-reframe-tests/config/cscs.py \\\n  -c cscs-reframe-tests/checks/apps/arbor/ --keep-stage-files \\\n  --dry-run\n</code></pre> <ul> <li><code>-C</code>: the system configuration, here provided by the CSCS test suite</li> <li><code>-c &lt;path&gt; -r</code>: the path containing checks to run. The <code>-r</code> flag will search recursively for tests in sub-directories<ul> <li>in this case, only consider the arbor tests.</li> <li><code>-c cscs-reframe-tests/checks</code> would search through all tests for tests that   are compatible with the features provided by the environments in the <code>extra/reframe.yaml</code> file.</li> </ul> </li> <li><code>--keep-stage-files</code>: this will keep all of the intermediate scripts and configuration<ul> <li>stored in the <code>stage</code> sub-directory of the current path.</li> <li>very useful for debugging problems with our tests.</li> </ul> </li> <li><code>--dry-run</code>: generate all the stage files and scripts without running the tests.</li> </ul> <p>Tip</p> <p>Use the <code>--dry-run</code> first, to inspect the generated job scripts and that there are no issues in the code. Once everything looks good, remove the flag to run the tests (which would take longer).</p> <p>Tip</p> <p>Look in the <code>stage</code> path that is created in the path where you called reframe for all of the job scripts, build files, and results.</p> <p>Tip</p> <p>If ReFrame tests fail because of <code>ReqNodesNotAvail</code> and you think it is a fluke, try setting <code>RFM_IGNORE_REQNODENOTAVAIL=y</code>.</p>"},{"location":"pkg-writing-docs/","title":"Writing Documentation","text":"<p>The documentation for uenv is written with Material for mkdocs.</p> <p>See the documentation for Material for details on all of the features that it provides.</p>"},{"location":"pkg-writing-docs/#getting-started","title":"Getting Started","text":"<p>To view the documentation as you write, you can install Material for mkdocs using pip.</p> <p>First, create Python virtual environment and install <code>mkdocs-material</code> <pre><code># create and activate a venv\npython3 -m venv pyenv\nsource pyenv/bin/activate\n\n# install required packages\npip install --upgrade pip\npip install mkdocs-material\n</code></pre></p> <p>Note</p> <p>If you just created the python virtual environment, you might have to restart it for the <code>mkdocs</code> executable to be added to <code>PATH</code>.</p> <pre><code># you have to deactivate and start again for mkdocs to be available\ndeactivate\nsource pyenv/bin/activate\n</code></pre> <p>The documentation is built using the <code>mkdocs</code> executable. To view your documentation in your browser, run the mkdocs server: <pre><code>mkdocs serve\n</code></pre></p> <p>Leave it running, and every time you save the markdown file the docs will be regenerated.</p> <p>The docs can be viewed by using the link printed by <code>mkdocs serve</code>:</p> <pre><code>mkdocs serve                                                                                                  (pyenv) main [e2006ad] \u0394 ?\nINFO    -  Building documentation...\nINFO    -  Cleaning site directory\nINFO    -  The following pages exist in the docs directory, but are not included in the \"nav\" configuration:\n             - writing-docs.md\nINFO    -  Documentation built in 0.15 seconds\nINFO    -  [12:17:14] Watching paths for changes: 'docs', 'mkdocs.yml'\nINFO    -  [12:17:14] Serving on http://127.0.0.1:8000/\n</code></pre> <p>And viewing it ( http://127.0.0.1:8000/ typically) in your local browser.</p>"},{"location":"pkg-writing-docs/#documentation-layout","title":"Documentation Layout","text":"<p>To add documentation for a uenv, create a file <code>docs/uenv-&lt;uenv name&gt;.md</code>, and look at existing documentation to get started.</p> <p>For your docs to be generated, it has to be added to the index in the <code>mkdocs.yml</code> file in the root of the repository. For example, to add docs for a new environment called <code>wombat</code>:</p> <pre><code>nav:\n  - Home: index.md\n  - 'uenv':\n    - 'Overview': uenv-overview.md\n    - 'wombat': uenv-wombat.md\n</code></pre>"},{"location":"tutorial-overview/","title":"uenv tutorials","text":"<p>A collection of tutorials and user-guides for uenv users on Alps.</p> <ul> <li>how to use uenv on Alps from the 2024 CSCS User Day.</li> <li>how to use Spack with uenv</li> </ul>"},{"location":"tutorial-spack/","title":"Using uenvs as upstream Spack instances","text":"<p>User-environments (uenvs) are built with Spack using the Stackinator tool. Therefore, a uenv is tightly coupled with Spack and can be used as an upstream Spack instance (see Chaining Spack Installations for more details).</p> <p>Note</p> <p>While this guide tries to explain everything step-by-step, it might be difficult to follow without any knowledge of Spack. Please have a look at Spack Basic Usage for a short introduction to Spack.</p> <p>The uenv of a supported application contains all the dependencies to build the software with a particular configuration. In Spack, such configuration is defined by a spec (see Spack Basic Usage for more details). Most uenvs already provide modules or Spack Filesystem Views which allow to manually build the same configuration. However, it is possible that you want to build or develop an application with a different configuration. To avoid re-building the whole uenv, you can re-use what is already there and build your new configuration using Spack. </p> <p>This guide explains a developer workflow allowing to either build your own package with Spack, or use Spack to build all the dependencies and provide a build environment for the package.</p> <p>Note</p> <p>This guide assumes that you have a local installation of Spack. If you don't have Spack installed, follow Spack Getting Started.</p> <p>Tip</p> <p>To avoid compatibility issues, try to match the version of your local Spack instance with the version of Spack of the uenv. You can use the following command to clone the same Spack version used by the uenv: <pre><code>git clone --filter=tree:0 $(jq -r .spack.repo /user-environment/meta/configure.json)\ngit -C spack checkout $(jq -r .spack.commit /user-environment/meta/configure.json)\n</code></pre></p> <p>Warning</p> <p>Avoid installing Spack on <code>HOME</code>. Packages are installed within the <code>spack/</code> folder, and you might quickly run out of space. Use <code>SCRATCH</code> instead.</p> <p>Danger</p> <p>The recommendation to use <code>SCRATCH</code> to install your local Spack instance(s) might change in the future. Make sure you are aware of our <code>SCRATCH</code> cleaning policy. </p>"},{"location":"tutorial-spack/#example-quantum-espresso","title":"Example: Quantum ESPRESSO","text":"<p>As an example, we will consider Quantum ESPRESSO as the application you want to develop. Let's assume that the provided configuration in the official uenv is the following:</p> <pre><code>quantum-espresso@7.3.1 %nvhpc +libxc +cuda cuda_arch=90\n</code></pre> <p>This spec defines a build of Quantum ESPRESSO version <code>7.3.1</code> using the <code>nvhpc</code> compiler. All other dependencies and features are defined by the default values in the Quantum ESPRESSO Spack package.</p>"},{"location":"tutorial-spack/#set-uenv-as-upstream-spack-instance","title":"Set uenv as upstream Spack instance","text":"<p>Here we assume the uenv described above is called <code>quantumespresso/v7.3.1</code> and it is already deployed. You can therefore pull the <code>quantumespresso/v7.3.1</code> image and start the uenv as follows:</p> <pre><code>uenv image pull quantumespresso/v7.3.1\nuenv start quantumespresso/v7.3.1\n</code></pre> <p>With the uenv active, you can now tell your local Spack instance to use the uenv as an upstream Spack instance (see Chaining Spack Installations for more details):</p> <pre><code>export SPACK_SYSTEM_CONFIG_PATH=/user-environment/config/\n</code></pre> <p>Note</p> <p>We assumed here that the uenv is mounted in the standard location <code>/user-environment/</code>. If it is mounted in a non-standard location, adjust the previous command accordingly.</p>"},{"location":"tutorial-spack/#building-your-own-version","title":"Building your own version","text":"<p>Let's assume you want to have a version of Quantum ESPRESSO with GPU-aware MPI:</p> <pre><code>quantum-espresso@7.3.1 %nvhpc +libxc +cuda cuda_arch=90 +mpigpu\n</code></pre> <p>This variant of Quantum ESPRESSO is not available in the uenv.</p>"},{"location":"tutorial-spack/#spack-environment","title":"Spack Environment","text":"<p>To make things clean and reproducible, you can use Spack Environments to describe what you want to build. To define a Spack environment you have to create the following file, named <code>spack.yaml</code>, in a folder (hereafter referred to as <code>SPACK_ENV_FOLDER</code>):</p> <pre><code>spack:\n  specs:\n  -  quantum-espresso@7.3.1 %nvhpc +libxc +mpigpu\n  packages:\n    all:\n      prefer:\n        - +cuda cuda_arch=90\n  view: false\n  concretizer:\n    unify: true\n</code></pre> <p><code>packages:all:prefer</code> indicates that you want the <code>+cuda</code> variant active for all packages that have it.</p> <p>Note</p> <p>It is good practice to have a single root spec in an environment, and to define constraint on packages or specific dependencies in the <code>packages:</code> field.</p> <p>Tip</p> <p>To create an environment you can also use <code>spack env create SPACK_ENV_FOLDER</code> and edit the <code>spack.yaml</code> file with <code>spack -e SPACK_ENV_FOLDER config edit</code>. Alternatively, you can use <code>spack -e SPACK_ENV_FOLDER add &lt;spec&gt;</code> to add root specs to the environment and <code>spack -e config add &lt;config&gt;</code> to add configutations to the environment. </p> <p>Example</p> <p>An example of creating an environment for building Quantum ESPRESSO: <pre><code>spack env create qe-env\nspack -e qe-env  add quantum-espresso%nvhpc +cuda               # Add spec for Quantum ESPRESSO\nspack -e qe-env config add packages:all:prefer:cuda_arch=90     # Add configuration for all packages \n</code></pre></p>"},{"location":"tutorial-spack/#building","title":"Building","text":"<p>After defining the environment above, you can concretize it:</p> <pre><code>spack -e SPACK_ENV_FOLDER concretize -f\n</code></pre> <p>The result of the concretization will be printed on screen. Packages marked with <code>-</code> are packages that will be freshly installed in your local Spack instance. Packages marked as <code>[^]</code> (upstream) are packages taken directly from the uenv (which is being used as upstream Spack instance). You should see many packages marked as <code>[^]</code>, which are being re-used from the uenv. <code>[e]</code> (external) are external packages that are already installed in the system (and are defined in the system configuration of the system for which the uenv is built). Finally, packages marked as <code>[+]</code> are packages that are already installed in your local Spack instances.</p> <p>Using the uenv as an upstream Spack instance will greatly speed up compilation, since Spack will have to build only a small subset of packages.</p> <p>You can finally build everything in the concretized environment:</p> <pre><code>spack -e SPACK_ENV_FOLDER install\n</code></pre>"},{"location":"tutorial-spack/#developing-with-spack-build-manually","title":"Developing with Spack (build manually)","text":"<p>In addition to wanting to build a different configuration of a package as described above, you might want to build your own development version of the software from source. Let's assume you want to develop Quantum ESPRESSO, with GPU-aware MPI:</p> <pre><code>quantum-espresso@7.3.1 %nvhpc +libxc +cuda cuda_arch=90 +gpumpi\n</code></pre>"},{"location":"tutorial-spack/#spack-environment-and-building-dependencies","title":"Spack Environment and Building Dependencies","text":"<p>As described above, you can define a Spack environment describing the version of the package you want to build and the constraints on the dependencies. After concretizing the environment with</p> <pre><code>spack -e SPACK_ENV_FOLDER concretize -f\n</code></pre> <p>you can tell Spack to only install the dependencies, since you want to build the root spec manually:</p> <pre><code>spack -e SPACK_ENV_FOLDER install --only=dependencies\n</code></pre>"},{"location":"tutorial-spack/#building-the-root-spec-manually","title":"Building the root spec manually","text":"<p>Finally, you are ready to build the root spec manually. With Spack you can get a shell within the build environment as follows:</p> <pre><code>spack -e SPACK_ENV_FOLDER build-env quantum-espresso -- bash\n</code></pre> <p>where <code>quantum-espresso</code> denotes the root spec. Since there is only one such spec, there is no need to explicitly write out the version nor the variants.</p> <p>Within the build environment, the software can be built using the provided build system. Quantum ESPRESSO uses CMake, therefore you can simply do the following:</p> <pre><code>mkdir build &amp;&amp; cd build\n\ncmake \\\n    -GNinja\n    -DQE_ENABLE_CUDA=ON \\\n    -DQE_ENABLE_MPI_GPU_AWARE=ON \\\n    -DQE_ENABLE_LIBXC=ON \\\n    ..\n\nninja -j 32\n</code></pre>"},{"location":"tutorial-spack/#developing-with-spack-build-with-spack","title":"Developing with Spack (build with Spack)","text":"<p>Spack already knows how to build Quantum ESPRESSO with CMake, therefore you could use Spack to build your development version for you.</p> <p>Warning</p> <p>Changes to CMake might require changes to the Quantum ESPRESSO Spack package.</p>"},{"location":"tutorial-spack/#spack-environment_1","title":"Spack Environment","text":"<p>You can create a Spack environment as suggested above:</p> <pre><code>spack env create qe-dev-env\nspack -e qe-dev-env add quantum-espresso%nvhpc +libxc +gpumpi\nspack -e $SCRATCH/qe-env config add packages:all:prefer:cuda_arch=90\n</code></pre> <p>In addition to adding Quantum ESPRESSO as a root spec, you have to tell Spack where to find the source code (and which version/branch it corresponds to). You can use the following command:</p> <pre><code>spack -e qe-dev-env develop -p PATH_TO_QE_SOURCE_CODE quantum-espresso@=develop\n</code></pre> <p>After concretizing the environment with</p> <pre><code>spack -e SPACK_ENV_FOLDER concretize -f\n</code></pre> <p>you can tell Spack to install everything, including Quantum ESPRESSO using the source code in <code>PATH_TO_QE_SOURCE_CODE</code>:</p> <pre><code>spack -e SPACK_ENV_FOLDER install\n</code></pre>"},{"location":"tutorial-spack/#known-limitations","title":"Known Limitations","text":"<p>Warning</p> <p>Swapping the upstream Spack instance by loading different uenvs might lead to surprising inconsistencies in the Spack database. If this happens, you can uninstall everything from your local Spack instance with <code>spack uninstall --all</code> and clean up with <code>spack clean --all</code>. To avoid this problem, you can also work with multiple local Spack instances (one for each uenv).</p>"},{"location":"tutorial-userday-2024/","title":"Using uenv to build and run applications on Alps","text":""},{"location":"tutorial-userday-2024/#logging-in","title":"logging in","text":"<p>Once you have set up your ssh token, an easy way to simplify login in is by updating your <code>~/.ssh/config</code>:</p> <pre><code>&gt; cat ~/.ssh/config\nHost ela\n    HostName ela.cscs.ch\n    ForwardAgent yes\n    User bcumming\n\nHost daint.alps\n    HostName daint-ln001.cscs.ch\n    ProxyJump ela\n    User bcumming\n</code></pre> <p>Then log into one of the systems using the alias defined in the config file: <pre><code>ssh todi\nssh daint.alps\n</code></pre></p>"},{"location":"tutorial-userday-2024/#setting-up-for-the-first-time","title":"setting up for the first time","text":"<p>First, check that uenv has been installed on the system: <pre><code>uenv --version\nuenv status\n</code></pre></p> <p>Before using uenv, a repository (repo) for storing downloaded (pulled) needs to be created.</p> <p>e.g. try the following command that lists the downloaded uenv: <pre><code>uenv image ls\n</code></pre></p> <p>Create a repo in the default location <pre><code># get help on the repo command\nuenv repo --help\n\n# find the status of the default repo\nuenv repo status\n\n# create the new repo\nuenv repo create\nfind $SCRATCH/.uenv-images\n</code></pre></p>"},{"location":"tutorial-userday-2024/#finding-and-pulling-images","title":"finding and pulling images","text":"<p>To search for the uenv that are provided by CSCS, use the <code>uenv image find</code> command when logged in:</p> <pre><code>uenv image find                 # all uenv on the current system\nuenv image find cp2k            # all cp2k images on the current system\nuenv image find cp2k/2024.2     # refine the search\nuenv image find cp2k/2024.2:v1  # refine the search\nuenv image find --system=eiger  # choose the system\nuenv iamge find --uarch=gh200   # choose the uarch\n</code></pre> <p>To download an image, use the <code>uenv image pull</code> command <pre><code>uenv image pull editors\n</code></pre></p> <p>There might be more than one uenv that matches the description. If so, be more specific:</p> <ul> <li>use the full disambiguated name</li> <li>use the image hash or id</li> <li>specify the uarch</li> </ul> <pre><code>uenv image pull editors/24.7:v1\nuenv image pull 95fc886e35a3b27f\nuenv image pull editors --system=daint\n</code></pre> <p>Look at the repository, to see what has been downloaded: <pre><code>find $SCRATCH/.uenv-images\n</code></pre></p> <p>Let's also pull another couple of uenv</p> <pre><code>uenv image pull prgenv-gnu/24.7:v3\n</code></pre>"},{"location":"tutorial-userday-2024/#inspecting-uenv","title":"inspecting uenv","text":"<p>Let's say we have pulled some some uenv, e.g.: <pre><code>&gt; uenv image ls\nuenv/version:tag                        uarch date       id               size\nprgenv-gnu/24.7:v3                      gh200 2024-08-23 b50ca0d101456970 3.8GB\neditors/24.7:v1                         gh200 2024-09-01 95fc886e35a3b27f 1.2GB\narbor/v0.9:1435230711                   gh200 2024-09-01 41fbf21853e82969 3.6GB\n</code></pre></p> <p>To get more information about a uenv, use the <code>uenv image inspect</code> command <pre><code>&gt; uenv image inspect prgenv-gnu\n</code></pre></p> <p>The output can be formatted using Jinja2 templates, (useful for automated workflows): <pre><code>&gt; uenv image inspect prgenv-gnu --format={sqfs}\n</code></pre> For example, print all uenv with their views: <pre><code>for id in $(uenv image ls --no-header | awk '{print $4}');\ndo\n    meta=$(uenv image inspect --format '{meta}' $id)/env.json;\n    echo \"$id : $(jq -j '.views | keys | join(\" \")' $meta)\";\ndone;\n</code></pre></p> <p>You can see a full list of format options: <pre><code>uenv image inspect --help\n</code></pre></p>"},{"location":"tutorial-userday-2024/#using-uenv","title":"using uenv","text":""},{"location":"tutorial-userday-2024/#starting-a-uenv","title":"starting a uenv","text":"<p>start running a uenv</p> <pre><code>uenv start prgenv-gnu\n</code></pre> <p>What happened? The squashfs image was mounted:</p> <pre><code>findmnt --mountpoint /user-environment\nls /user-environmment\n</code></pre> <p>uenv provides a command for checking whether a uenv is running: <pre><code>&gt; uenv status\n/user-environment:prgenv-gnu\n  GNU Compiler toolchain with cray-mpich, Python, CMake and other development tools.\n  views:\n    default\n    modules: activate modules\n    spack: configure spack upstream\n</code></pre></p> Info <p>The <code>uenv start</code> and <code>uenv run</code> commands (see below) create a new process and mount the image so that that process is the only one that can read the mounted image. This isolates different users and login sessions on the same compute/login node - different users can have their own uenv mounted at the same location without affecting one another.</p> <p>The software has been mounted, but it is not yet available in the environment: <pre><code>gcc --version\nwhich gcc\ncmake --version\nwhich cmake\npython3 --version\nwhich python3\n</code></pre> Python 3.6 is the default provided by SLES - not good enough for the discerning HPC user!</p> <p>On HPC systems, software is usually \"loaded\" using modules. The software packages are installed in subdirectories, e.g. <code>netcdf</code> provided by CPE: <pre><code>/opt/cray/pe/netcdf/4.9.0.9/gnu/12.3\n/opt/cray/pe/netcdf/4.9.0.9/crayclang/17.0\n</code></pre> Each of these contains a <code>bin</code>, <code>include</code>, and <code>lib</code> paths.</p> <p><code>module load netcdf</code> will set environment variables <code>PATH</code>, <code>LD_LIBRARY_PATH</code>, etc - making the software available.</p>"},{"location":"tutorial-userday-2024/#uenv-views-modify-the-environment","title":"uenv views modify the environment","text":"<pre><code># leave the previous uenv session\nexit\n\n# start with a view activated\nuenv start prgenv-gnu --view=default\n\n# now the software is available\ncmake --version\nwhich cmake\npython3 --version\nwhich python3\ngcc --version\nwhich gcc\nnvcc --version\nwhich nvcc\n</code></pre> <p>Let's have a closer look at <code>/user-environment/env/default/</code></p>"},{"location":"tutorial-userday-2024/#can-i-mix-and-use-multipe-uenv-at-the-same-time","title":"Can I mix and use multipe uenv at the same time?","text":"<p>A difference between uenv and modules are that setting up the environment is more \"declarative\" than \"imperative\":</p> <ul> <li>the images to mount and views to start are specified at the start of a session</li> <li>after they are set, they can't change<ul> <li>if you use the <code>modules</code> view, it is possible to <code>load</code>, <code>swap</code>, <code>purge</code> etc as in the days of yore.</li> </ul> </li> </ul> <p>It is not possible to run \"uenv inside uenv\"</p> <ul> <li>the first reason is security</li> <li>each uenv has a fixed mount point - only one image can be mounted at a location</li> </ul> <p>This can be frustrating compared to using modules, when experimenting and testing on the command line:</p> <ul> <li>swapping uenv, views, etc requires going back to the start</li> <li>when using modules, one can continuously load, unload, swap, purge modules while interacting with the shell.</li> </ul> <p>We might also be used to putting code like this in <code>.bashrc</code> to set up a \"default\" environment: <pre><code>module swap PrgEnv-cray PrgEnv-gnu\nmodule swap gcc/12.3.0 module load cudatoolkit\n</code></pre></p> <p>An equivalent approach with <code>uenv</code> is not possible <pre><code># the following starts a new shell\n# ... and once loaded, we can't load a different shell\nuenv start my-default-environment --view=work\n</code></pre></p> <p>The benefit of this approach is that reproducing a working environment is simpler:</p> <ul> <li><code>uenv start cp2k/2024.2,editors --view=cp2k:develop,ed</code> describes the environment</li> </ul> <p>This will hopefully reduce the frequency of one of the main challenges when reproducing issues affecting our users</p> <ul> <li>long-forgotten module commands in bashrc</li> <li>\"this worked yesterday when this combination of modules was loaded in this order\"</li> </ul> Note <p>Please report pain points in your workflow.</p>"},{"location":"tutorial-userday-2024/#it-is-possible-to-use-more-than-one-uenv-simultaneously","title":"It is possible to use more than one uenv simultaneously","text":"<p>It is possible to mount more than one uenv in the same session, so long as they are both configured \"up front\" when the session is started.</p> <p>Examples for this use case include:</p> <ul> <li>using a profiler or debugger uenv alongside a programming environment or application uenv</li> <li>mounting useful utilities (e.g. editors) alongside a programming environment.</li> </ul> <pre><code># start two uenv in the same session\nuenv start prgenv-gnu editors\n\n# provide custom mount points\nuenv start prgenv-gnu:/user-tools editors:/user-environment\nuenv status\n\n# start two uenv in the same session with views\nuenv start prgenv-gnu editors --view=prgenv-gnu:modules,ed\n\n# disambiguate the view name\nuenv start prgenv-gnu editors --view=prgenv-gnu:modules,ed\n</code></pre>"},{"location":"tutorial-userday-2024/#why-are-there-both-uenv-run-and-uenv-start","title":"why are there both <code>uenv run</code> and <code>uenv start</code>","text":"<p>The <code>run</code> command runs a single command in the configured environment, returning us to the unmodified calling shell.</p> <p>This is useful to use more than one uenv in a workflow - run each step with a different uenv:</p> <pre><code>uenv run --view=default prgenv-gnu -- ./pre-process.sh\nuenv run --view=cp2k cp2k -- ./cp2k-simulation.sh\nuenv run --view=default prgenv-gnu -- ./post-process.sh\n</code></pre> <p>Another, slightly irreverant, example is to make emacs available. <pre><code>which emacs\n</code></pre></p> <p>Emacs is provided by the <code>editors</code> uenv (access through the <code>ed</code> view): <pre><code>uenv run --view=ed editors:/user-tools -- emacs\n</code></pre></p> <p>The above command works, but is not very practical for regular use. Instead:</p> <pre><code>alias emacs='uenv run --view=ed editors:/user-tools -- emacs'\nemacs test.cpp\n</code></pre> <p>it seems odd, but by adding that alias to your <code>.bashrc</code>, you now have emacs available without installing any software, or modifying your environment (keep it simple).</p>"},{"location":"tutorial-userday-2024/#building-software-using-uenv","title":"building software using uenv","text":""},{"location":"tutorial-userday-2024/#a-simple-application-using-prgenv-gnu","title":"a simple application using prgenv-gnu","text":"<p>Here we build <code>affinity</code>, a small MPI+CUDA aware application for reporting CPU+GPU affinity.</p> <p>A view provides all of the requirements (gcc, MPI, cuda, cmake, ninja) <pre><code>uenv start prgenv-gnu/24.7 --view=default\ngit clone git@github.com:bcumming/affinity.git\ncd affinity\nmkdir build\ncd build\nwhich cmake\nwhich mpicc\nnvcc --version\nCC=mpicc CXX=mpicxx cmake .. -GNinja\nninja\n\nOMP_NUM_THREADS=4 srun -n4 -N1 --gpus-per-task=1 ./affinity.mpi\nsrun -n4 -N1 --gpus-per-task=1 ./affinity.cuda\n</code></pre></p> <p>The uenv provides cray-mpich, with some key differences:</p> <ul> <li>the MPI compilers are <code>mpicc</code>, <code>mpicxx</code>, <code>mpifort</code><ul> <li>replacing the <code>CC</code>, <code>cc</code>, <code>ftn</code> compiler wrappers provided by CPE</li> </ul> </li> <li>dependencies required for GPU-aware MPI are hard coded (no need to load specific modules)</li> </ul> <pre><code>vim $(which mpicc)\n</code></pre> <p>modules are also available in most uenv:</p> <pre><code>uenv start prgenv-gnu/24.7 --view=modules\nmodule avail\nmodule load cray-mpich cmake cuda gcc ninja\n\n# then run cmake, ninja, as before\n</code></pre> <p>The <code>modules</code> view simply updates <code>MODULEPATH</code>: <pre><code>{\n  \"list\": {\n    \"MODULEPATH\": [\n      {\n        \"op\": \"prepend\",\n        \"value\": [\n          \"/user-environment/modules\"\n        ]\n      }\n    ]\n  },\n  \"scalar\": {}\n}\n</code></pre></p>"},{"location":"tutorial-userday-2024/#a-more-complicated-example","title":"A more complicated example","text":"<p>Use a view to build Arbor, a neuroscience application that can optionally use MPI, CUDA and Python.</p> <p>We use a uenv that was developed for Arbor using a uenv recipe.</p> <pre><code>git clone --depth=1 --branch=v0.9.0 git@github.com:arbor-sim/arbor.git\nmkdir build\ncd build\n\nexport CC=`which gcc`\nexport CXX=`which g++`\ncmake ../arbor -DARB_GPU=cuda -DARB_WITH_MPI=on -DARB_WITH_PYTHON=on -GNinja -DCMAKE_CUDA_ARCHITECTURES=90\nninja examples pyarb unit\n\n# test the build\n./unit\n./bin/busyring\nexport PYTHONPATH=$PWD/python:$PYTHONPATH\npython -c 'import arbor; print(arbor.config)'\n</code></pre>"},{"location":"tutorial-userday-2024/#setting-up-a-python-venv","title":"setting up a Python venv","text":"<p>Create a virtual environment that uses the software in a uenv as the starting point: <pre><code>uenv start arbor --view=develop\n\n# check the version of python that is being used\nwhich python\npython --version\n\n# create a venv\npython -m venv --system-site-packages env\nsource env/bin/activate\npip list\n</code></pre></p>"},{"location":"tutorial-userday-2024/#building-using-spack","title":"building using Spack","text":"<p>Spack is a popular tool for installing scientific software:</p> <ul> <li>configure the software to install <code>arbor@v0.10.0+mpi+cuda+python</code></li> <li>Spack will build all missing dependencies</li> </ul> <p>Each uenv image provides a standard Spack configuration</p> <ul> <li>Can be used as the basis for your own spack environment</li> </ul> <p>The spack configuration can be accessed using the <code>spack</code> view:</p> <pre><code>&gt;uenv start prgenv-gnu --view=spack\nloading the view prgenv-gnu:spack\n&gt; printenv | grep UENV_SPACK\nUENV_SPACK_COMMIT=bd66dce2d668a6234504594661506cdd1eaca4adc\nUENV_SPACK_CONFIG_PATH=/user-environment/config\nUENV_SPACK_URL=https://github.com/spack/spack.git\n&gt; git clone $UENV_SPACK_URL\n&gt; cd spack\n&gt; cat /user-environment/meta/recipe/config.yaml\nname: prgenv-gnu\nspack:\n  commit: releases/v0.22\n  repo: https://github.com/spack/spack.git\nstore: /user-environment\ndescription: GNU Compiler toolchain with cray-mpich, Python, CMake and other development tools.\n&gt; git switch releases/v0.22\n&gt; ./bin/spack find\n&gt; ./bin/spack -C $UENV_SPACK_CONFIG_PATH find\n</code></pre> <p>See uenv documentation for how to use the packages installed in a uenv with Spack to install your own software:</p> <p>https://eth-cscs.github.io/alps-uenv/uenv-compilation-spack/</p>"},{"location":"tutorial-userday-2024/#applications-provided-by-uenv","title":"Applications provided by uenv","text":"<p>So far our examples have provided examples of using common CLI tools and compilers provided by uenv to work in the terminal and build scientific software.</p> <p>Uenv can also provide scientific software and tools like debuggers - no compilation necessary.</p> <p>There are application uenv provided by CSCS for common scientific codes:</p> <ul> <li>CP2K</li> <li>GROMACS</li> <li>NAMD</li> <li>LAMMPS</li> <li>ICON</li> <li>VASP</li> <li>quantumespresso</li> <li>and more ...</li> </ul> <p>Scientific software is diverse, so there is no hard and fast rule for which views will be provided. <pre><code>quantumespresso : default develop modules spack\nnamd            : namd namd-single-node develop develop-single-node modules spack\ncp2k            : cp2k develop modules spack\narbor           : arbor develop modules spack\n</code></pre></p> <p>Typically, the uenv provide two types of view:</p> <ul> <li>an <code>application view</code> that provides the application<ul> <li>\"ready to run\"</li> <li>like <code>module load &lt;application&gt;</code></li> <li>use this to directly use the version of the software provided by CSCS</li> </ul> </li> <li>a <code>development view</code> that provides all of the application's dependencies<ul> <li>use this to build your own version of the software</li> <li>e.g. if you need a different version or configuration</li> </ul> </li> </ul>"},{"location":"tutorial-userday-2024/#slurm","title":"slurm","text":"<p>So far we have looked at using <code>uenv start</code> and <code>uenv run</code> to interact with uenv. Both of these approaches load the uenv on the node that the command is run on. This is an important detail - the uenv is not mounted on compute nodes, and is only visible to the current process.</p> <p>We can test that: <pre><code># by default, nothing mounted on the compute node\nsrun -n1 ls /user-environment\n\n# start a uenv on the login node and try again\nuenv start prgenv-gnu\nsrun -n1 ls /user-environment/\n</code></pre></p> <p>The image was mounted on the compute node - what is going on?</p> <p>There is a uenv slurm plugin that will handle mounting uenv on compute nodes. By default, if</p> <ul> <li>no <code>--uenv</code> flag is passed</li> <li>a uenv was mounted on the login node when srun was called</li> </ul> <p>The plugin will mount the image automatically on the login node</p> <ul> <li>related: why are the modules loaded when calling sbatch on a login node loaded on all the compute nodes?</li> </ul> <p>Using <code>uenv start</code> and <code>uenv run</code> inside an sbatch job script has downsides:</p> <ul> <li>it increases the complexity of the job script</li> <li>it may be inefficient in some cases<ul> <li>e.g. 128 ranks on a single node all mount the same uenv image</li> </ul> </li> </ul> <p>The uenv Slurm plugin automates the process of starting uenv</p> <ul> <li>the uenv to load is provided using a <code>--uenv</code> option</li> <li>the plugin then:<ol> <li>validates the request on the login node (when srun or sbatch is called)</li> <li>mounts the uenv on each compute node before the individual MPI ranks are created</li> </ol> </li> <li>the uenv is automatically removed when each <code>srun</code> command finishes.</li> </ul> <p>From the perspective of the script running on the compute node, the uenv is always mounted.</p> <p>A simple example using <code>srun</code> <pre><code># start a shell on a compute node with the uenv mounted\nsrun -n1 --uenv=prgenv-gnu/24.7:v3 --pty bash\n\n\n# run a command on a compute node with the uenv mounted\nsrun -n1 --uenv=prgenv-gnu/24.7:v3 bash -c 'uenv view modules; module avail'\n</code></pre></p>"},{"location":"tutorial-userday-2024/#using-uenv-in-sbatch-jobs","title":"using uenv in sbatch jobs","text":"<p>The <code>--uenv</code> flag can also be used in slurm batch jobs:</p> <pre><code>#!/bin/bash\n#SBATCH --uenv=prgenv-gnu/24.7:v3\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=4\n#SBATCH --output=job%j.out\n#SBATCH --error=job%j.out\n\n# load the view\nuenv view default\nexe=/capstor/scratch/cscs/bcumming/demo/affinity/build/affinity.cuda\n\n# the uenv is available inside the script\nwhich nvcc\nuenv status\n\n# and also automatically mounted on the compute nodes:\nsrun -n4 -N1 $exe\n\n# note: within this week the following will be possible\n\n#srun -n4 -N1 --uenv=prgenv-gnu --uenv-view=default affinity.gpu\n</code></pre> <p>Sometimes a job will require multiple uenv, e.g. pre-processing and post-processing stages might use one uenv, and the simulation runs would use an application uenv. The uenv specified in the <code>#SBATCH --uenv=</code> comment can be overriden in individual calls to <code>srun</code> inside the script. For example, run a job that first uses the affinity application that we built earlier with <code>prgenv-gnu</code>, then run an arbor benchmark that was built with the <code>arbor</code> uenv.</p> <pre><code>#!/bin/bash\n#SBATCH --uenv=prgenv-gnu/24.7:v3\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=4\n#SBATCH --output=job%j.out\n#SBATCH --error=job%j.out\n\n# load the view\nuenv view default\nexe=/capstor/scratch/cscs/bcumming/demo/affinity/build/affinity.cuda\n\nsrun -n4 -N1 $exe\n\narbor=/capstor/scratch/cscs/bcumming/demo/arbor/build/bin/busyring\nsrun --uenv=arbor/v0.9 -n4 -N1 --gpus-per-task=1 $arbor\n</code></pre> Note <p>The <code>--uenv</code> slurm flag is a little awkward to use at the moment because there is no corresponding <code>--view</code> flag. This forces us th use <code>uenv view</code> command separately, which is messy and can create significant complexity in scripts.</p> <p>A new version of the slurm plugin will be deployed on Daint and Todi very soon - hopefully this week - that will provide the <code>--uenv-view</code> flag.</p>"},{"location":"uenv-arbor/","title":"Arbor UENV","text":"<p>Arbor is a library for implementing performance portable network simulations of multi-compartment neuron models. An installation guide and library documentation are available online at docs.arbor-sim.org.</p> <p>The latest version of Arbor is provided on Todi and Eiger.</p> <p>Attention</p> <p>Arbor is not officially supported software at CSCS, and the Arbor uenv is provided \"as is\". CSCS we will try to help users where possible, however requests for model building support and advanced usage should be sent to Arbor Github issues.</p>"},{"location":"uenv-arbor/#getting-arbor","title":"Getting Arbor","text":"<p>The Arbor uenv can be pulled using the. To see which versions are available on a system, then download:</p> <pre><code># list all available versions and releases\nuenv image find arbor\n\n# pull a specific version\nuenv image pull arbor/v0.9:v1\n</code></pre>"},{"location":"uenv-arbor/#using-arbor","title":"Using Arbor","text":""},{"location":"uenv-arbor/#building-arbor-from-source","title":"Building Arbor from Source","text":"<p>Arbor can be built from source using the <code>develop</code> view, which provides all of the dependencies.</p> <p>Below is an example of building the most up-to0date version of Arbor, in <code>master</code> branch of the GitHub repository.</p> gh200zen2 and zen3 <pre><code># set up the environment\nuenv start --view=develop arbor/v0.9:v1\n\n# clone arbor and set up build path\ngit clone --recursive https://github.com/arbor-sim/arbor.git\nmkdir build &amp;&amp; cd build\n\n# configure and build\nCXX=mpicxx CC=mpicc cmake ../arbor -DCMAKE_CUDA_ARCHITECTURES=90 -DARB_WITH_MPI=on -DARB_GPU=cuda -DARB_WITH_PYTHON=on -DARB_VECTORIZE=on -DARB_SVE_WIDTH=128 -GNinja\nninja tests examples pyarb\n</code></pre> <p>The <code>-DCMAKE_CUDA_ARCHITECTURES=90</code> flag is required to generate code that is compatible with the Hopper GPU.</p> <pre><code># set up the environment\nuenv start --view=develop arbor/v0.9:v1\n\n# clone arbor and set up build path\ngit clone --recursive https://github.com/arbor-sim/arbor.git\nmkdir build &amp;&amp; cd build\n\n# configure and build\nCXX=mpicxx CC=mpicc cmake ../arbor -DARB_WITH_MPI=on -DARB_GPU=cuda -DARB_WITH_PYTHON=on -DARB_VECTORIZE=on -GNinja\nninja tests examples pyarb\n</code></pre> <p>Note</p> <p>A recursive checkout of Arbor is required to get the LLNL-units dependency, which is not provided by Spack.</p>"},{"location":"uenv-cp2k/","title":"CP2K","text":"<p>This page has moved to the CSCS Documentation.</p>"},{"location":"uenv-editors/","title":"editors","text":"<p>Provides text editors and useful command line tools, mounted at <code>/user-tools</code>.</p> <p>Warning</p> <p>The image has to be mounted at <code>/user-tools</code>, not the default <code>/user-environment</code> location. If loading the image alone, explicitly mount it at the right location <pre><code># if using v5 or lower uenv explicitly provide the mount point\n&gt; uenv start editors:/user-tools --view=ed\n\n# if using v6 or later uenv the mount point is automatically inferred\n&gt; uenv start editors --view=ed\n</code></pre> If loading the image alongside another (in this example we use the modules view, see below): <pre><code># if using v5 or lower uenv:\n&gt; uenv start prgenv-gnu/24.7:v3 editors --view=editors:modules\n\n# if using v6 or later uenv:\n&gt; uenv start prgenv-gnu/24.7:v3,editors --view=editors:modules\n</code></pre></p>"},{"location":"uenv-editors/#packages","title":"packages","text":"<p>Text editors:</p> <ul> <li>emacs</li> <li>nano</li> <li>neovim</li> <li>vim</li> </ul> <p>Useful command line tools:</p> <ul> <li>fd</li> <li>direnv</li> <li>lazygit</li> <li>node-js</li> <li>ripgrep</li> <li>screen</li> <li>tree-sitter</li> </ul> <p>Compilers and languages:</p> <ul> <li>lua</li> <li>python</li> <li>go</li> <li>rust</li> </ul>"},{"location":"uenv-editors/#interfaces","title":"interfaces","text":"<p>The uenv provides both a view called <code>ed</code> that will make all of the tools available. For example: <pre><code>&gt; uenv start editors:/user-tools --view=ed\nloading the view editors:ed\n&gt; which nvim\n/user-tools/env/ed/bin/nvim\n&gt; which emacs\n/user-tools/env/ed/bin/emacs\n&gt; which fd\n/user-tools/env/ed/bin/fd\n</code></pre></p> <p>Alternatively, the modules interface can be used to load individual tools: <pre><code>&gt; uenv start editors:/user-tools --view=modules\nloading the view editors:ed\n&gt; module avail\n\n----------------- /user-tools/modules ------------------\n   direnv    go         neovim     rust\n   emacs     lazygit    node-js    screen\n   fd        lua        python     tree-sitter\n   gcc       nano       ripgrep    vim\n\n&gt; module load screen\n&gt; screen --version\nScreen version 4.09.01 (GNU) 20-Aug-23\n</code></pre></p>"},{"location":"uenv-editors/#releases","title":"releases","text":""},{"location":"uenv-editors/#247","title":"24.7","text":"<ul> <li><code>v1</code>: did not provide neovim</li> <li><code>v2</code>: same as v1 with neovim added (fixed a libtool bug that caused an error when building the <code>libvterm</code> dependency of neovim)</li> </ul>"},{"location":"uenv-editors/#systems","title":"systems","text":"<p>The uenv is designed for deployment on all vClusters - it does not have any GPU-specific code and all of the tools it provides can be built on x86 and ARM CPUs.</p> <p>To find which versions (if any) are available on your target system: <pre><code>uenv image find editors\n</code></pre></p>"},{"location":"uenv-gromacs/","title":"GROMACS","text":"<p>This page has moved to the CSCS Documentation.</p>"},{"location":"uenv-lammps/","title":"LAMMPS","text":"<p>LAMMPS is a classical molecular dynamics code with a focus on materials modeling. It's an acronym for Large-scale Atomic/Molecular Massively Parallel Simulator. LAMMPS has potentials for solid-state materials (metals, semiconductors) and soft matter (biomolecules, polymers) and coarse-grained or mesoscopic systems. It can be used to model atoms or, more generically, as a parallel particle simulator at the atomic, meso, or continuum scale. See LAMMPS Features for a detailed overview.</p>"},{"location":"uenv-lammps/#running","title":"Running","text":"<p>We provide two versions of LAMMPS, one with the kokkos package enabled, and one with the GPU packaged enabled. These can be loaded as follows: <pre><code>uenv start --view kokkos &lt;LAMMPS_UENV&gt;\n</code></pre></p> <p>or</p> <pre><code>uenv start --view gpu &lt;LAMMPS_UENV&gt;\n</code></pre> <p>Warning</p> <p>LAMMPS is built with GPU-aware MPI. Make sure to set <code>MPICH_GPU_SUPPORT_ENABLED=1</code> when running LAMMPS.</p>"},{"location":"uenv-lammps/#building-from-source","title":"Building from source","text":"<p>The LAMMPS <code>uenv</code> provides all the dependencies required to build LAMMPS from source. You can follow these steps to build LAMMPS from source:</p> <pre><code># Start uenv and load develop view\nuenv start --view develop-kokkos &lt;LAMMPS_UENV&gt; # or uenv start --view develop-gpu &lt;LAMMPS_UENV&gt;, if building using the GPU package\n\n# cd to LAMMPS source directory\ncd &lt;PATH_TO_LAMMPS_SOURCE&gt;\n\n# CMake\nmkdir build &amp;&amp; cd build\ncmake -C ../cmake/presets/kokkos-cuda.cmake ../cmake/  -DKokkos_ENABLE_IMPL_CUDA_MALLOC_ASYNC=OFF -DKokkos_ARCH_NATIVE=ON -DKokkos_ARCH_HOPPER90=ON -DKokkos_ARCH_PASCAL60=OFF\n\ncmake --build . --parallel 32\n</code></pre>"},{"location":"uenv-linaro-forge/","title":"Linaro Forge tools","text":"<p>Documentation has been moved to the CSCS Documentation</p>"},{"location":"uenv-namd/","title":"NAMD","text":"<p>This page has moved to the CSCS Documentation.</p>"},{"location":"uenv-overview/","title":"Supported uenv on Alps","text":"<p>On Alps CSCS provides supported uenv that serve different application and development environment use cases.</p> <p>The list of supported uenv on a specific Alps system (<code>eiger</code>, <code>clariden</code>, <code>santis</code> etc) is documented in the CSCS knowledge base. Please read the knowledge base documentaiton for more details on specific systems.</p> <p>These pages here present detailed documentation for each of the officially supported uenv.</p> <ul> <li>CP2K is provided by <code>cp2k</code></li> <li>GROMACS is provided by <code>gromacs</code></li> <li>Linaro Forge is provided by <code>linaro-forge</code></li> <li>NAMD is provided by <code>namd</code></li> <li>Quantum Espresso is provided by <code>quantumespresso</code></li> </ul>"},{"location":"uenv-prgenv-fortran/","title":"prgenv-nvfortran uenv","text":"<p>Provides a small set of tools and libraries for building applications that need the NVIDIA Fortran compiler:</p> <ul> <li>applications that use OpenACC for GPU acceleration</li> <li>applications that use CUDA Fortran for GPU acceleration</li> </ul> <p>It provides the NVHPC compilers, MPI (cray-mpich), Python, cuda (on systems with NVIDIA GPUs).</p>"},{"location":"uenv-prgenv-fortran/#using-the-uenv","title":"Using the uenv","text":"<p>The image is only provided on Alps systems that have NVIDIA GPUs. To see which versions have been installed on a system, log in then search:</p> <pre><code># search for uenv\nuenv image find prgenv-nvfortran\n\n# pull a version\nuenv image find prgenv-nvfortran/24.11:v1\n</code></pre> <p>To use the uenv, we recommend using the uenv view <code>--view=nvfort</code>:</p> <pre><code>uenv start prgenv-nvfortran/24.11:v1 --view=nvfort\nmpif90 --version\n</code></pre> <p>The above example shows that the MPI compiler wrappers are using the underlying NVIDIA compiler. The following wrappers are available:</p> <ul> <li><code>mpif77</code></li> <li><code>mpif90</code></li> <li><code>mpifort</code></li> </ul>"},{"location":"uenv-prgenv-gnu/","title":"prgenv-gnu uenv","text":"<p>Provides a small set of tools and libraries built around the GNU compiler toolchain.</p> <p>It provides the GCC compilers (gcc, g++ and gfortran), MPI (cray-mpich), Python, cuda (on systems with NVIDIA GPUs).</p> <p>The following packages are provided:</p> <ul> <li><code>aws-ofi-nccl</code></li> <li><code>boost</code></li> <li><code>cmake</code></li> <li><code>cray-mpich</code><ul> <li>built with <code>cuda</code> support on systems with NVIDIA GPUs</li> </ul> </li> <li><code>cuda</code><ul> <li>only on systems with NVIDIA GPUs</li> </ul> </li> <li><code>fftw</code></li> <li><code>fmt</code></li> <li><code>gcc</code></li> <li><code>gsl</code></li> <li><code>hdf5</code></li> <li><code>kokkos</code></li> <li><code>kokkos-kernels</code></li> <li><code>kokkos-tools</code></li> <li><code>libtree</code></li> <li><code>meson</code></li> <li><code>nccl-tests</code></li> <li><code>nccl</code></li> <li><code>ninja</code></li> <li><code>openblas</code><ul> <li>built with OpenMP support</li> </ul> </li> <li><code>osu-micro-benchmarks</code></li> <li><code>python</code><ul> <li>a recent version of python 3</li> </ul> </li> </ul>"},{"location":"uenv-prgenv-gnu/#changelog","title":"Changelog","text":""},{"location":"uenv-prgenv-gnu/#2411","title":"24.11","text":"<ul> <li>Added GSL</li> <li>Added Boost with Chrono, Filesystem, Iostreams, MPI, Python, Regex, Serialization, System, Timer</li> <li>Added Kokkos with the CUDA, OpenMP, and Serial execution spaces</li> <li>Added Kokkos Kernels with explicit template instantiations and support for the most commonly used third party libraries</li> <li>Added Kokkos Tools</li> <li>Added PAPI</li> <li>Added SuperLU</li> <li>Added netlib-scalapack</li> <li>Added Lua</li> <li>Added lz4</li> <li>Added zlib-ng</li> <li>Added C++ and Fortran support to HDF5</li> <li>Updated CUDA to 12.6</li> <li>Changed aws-ofi-nccl to 1.9.2</li> </ul>"},{"location":"uenv-prgenv-gnu/#how-to-use","title":"How to use","text":"<p>The environment is designed as a fairly minimal set of </p> <p>There are three ways to access the software provided by prgenv-gnu, once it has been started.</p> viewsmodulesspack <p>The simplest way to get started is to use the file system view. A single view is provided:</p> <ul> <li>before v24.7: the <code>default</code> view</li> <li>since v24.7: the <code>develop</code> view</li> </ul> <pre><code># set when starting the uenv\nuenv start --view=develop prgenv-gnu/24.7:v1\n\n# example: the python executable provided by the uenv will be available\nwhich python\n/user-environment/env/default/bin/python\n\n# example: the python version is more recent that the 3.6 version in /usr/bin\npython --version \nPython 3.12.1\n</code></pre> <p>The uenv provides modules for all of the software packages. The modules are not available by default when a uenv starts, and have to be enabled.</p> <pre><code># with v4 of uenv:\nuenv start prgenv-gnu/24.7\nuenv modules use\n\n# with v5 of uenv:\n\n# method 1: enable modules when the uenv is started\nuenv start --view=modules prgenv-gnu/24.7\n\n# method 2: enable modules after the uenv has started\nuenv start prgenv-gnu/24.7\nuenv view modules\n\n# with v7 of uenv:\nuenv start --view=modules prgenv-gnu/24.7\n</code></pre> <p>To use Spack, you can check the guide for using Spack with uenv.</p> <p>Note</p> <p>If using the most recent release of uenv and a compatible uenv, load the <code>spack</code> view:</p> <pre><code># start the uenv with the spack view\n# note: the version might differ from the one in this example\nuenv start --view=spack prgenv-gnu/24.7:v1\n</code></pre> <p>Loading the <code>spack</code> view sets the following environment variables (with example values):</p> <pre><code>UENV_SPACK_CONFIG_PATH  /user-environment/config\nUENV_SPACK_URL          https://github.com/spack/spack.git\nUENV_SPACK_COMMIT       b5fe93fee1eec46a0750bd340198bffcb92ff9eec\n</code></pre>"},{"location":"uenv-prgenv-gnu/#platform-specific-hints","title":"platform specific hints","text":"gh200multicore <p>The version of MPI (cray-mpich) provided on Grace Hopper systems (Todi at the time of writing) supports GPU-direct or GPU aware communication, whereby pointers to GPU buffers can be passed directly to MPI calls. No special steps have to be taken to compile your code, however the following environment variable must be set to run an application that uses GPU pointers:</p> <pre><code>export MPICH_GPU_SUPPORT_ENABLED=1\n</code></pre> <p>There are no platform-specific notes for multicore.</p>"},{"location":"uenv-prgenv-nvfortran/","title":"prgenv-nvfortran uenv","text":"<p>Provides a small set of tools and libraries for building applications that need the NVIDIA Fortran compiler:</p> <ul> <li>applications that use OpenACC for GPU acceleration;</li> <li>applications that use CUDA Fortran for GPU acceleration.</li> </ul> <p>It provides the NVHPC compilers, MPI (cray-mpich), Python, cuda</p> <p>The naming scheme is <code>prgenv-nvidia/&lt;version&gt;:v&lt;i&gt;</code>, where <code>&lt;version&gt;</code> matches the version of the NVIDIA HPC SDK.</p> <ul> <li>the SDK is released every two months, and is numbered in the <code>YY.MM</code> format, e.g. <code>24.1</code> and <code>24.11</code>.</li> <li>the <code>prgenv-nvfortran</code> will be released three times a year (every second NVHPC release).</li> </ul>"},{"location":"uenv-prgenv-nvfortran/#using-the-uenv","title":"Using the uenv","text":"<p>The image is only provided on Alps systems that have NVIDIA GPUs. To see which versions have been installed on a system, log in then search:</p> <pre><code># search for uenv\nuenv image find prgenv-nvfortran\n\n# pull a version\nuenv image find prgenv-nvfortran/24.11:v1\n</code></pre> <p>To use the uenv, we recommend using the uenv view <code>--view=nvfort</code>:</p> <pre><code>uenv start prgenv-nvfortran/24.11:v1 --view=nvfort\nmpif90 --version\n</code></pre> <p>The above example shows that the MPI compiler wrappers are using the underlying NVIDIA compiler. The following wrappers are available:</p> <ul> <li><code>mpif77</code></li> <li><code>mpif90</code></li> <li><code>mpifort</code></li> </ul> <p>And the following C/C++ wrapers are available:</p> <ul> <li><code>mpicc</code></li> <li><code>mpicxx</code></li> </ul>"},{"location":"uenv-qe/","title":"Quantum ESPRESSO","text":"<p>This page has moved to the CSCS Documentation</p>"},{"location":"uenv-vasp/","title":"VASP","text":"<p>This page has moved to the CSCS Documentation.</p>"}]}